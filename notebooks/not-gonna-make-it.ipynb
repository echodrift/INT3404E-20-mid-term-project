{"metadata":{"colab":{"provenance":[],"collapsed_sections":["irlgl5eOtqD3","S0LJBZtGtsq7","TL89OJp16vnf"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Prep","metadata":{"id":"B32Yd-hf-NW9"}},{"cell_type":"code","source":"# to download zip files\n!pip install gdown","metadata":{"id":"LHqBYNxk7mfn","outputId":"af711ed8-fdab-4e9f-a00e-6041ded92c01","scrolled":true,"execution":{"iopub.status.busy":"2024-05-27T00:18:20.483732Z","iopub.execute_input":"2024-05-27T00:18:20.484024Z","iopub.status.idle":"2024-05-27T00:18:34.551613Z","shell.execute_reply.started":"2024-05-27T00:18:20.483996Z","shell.execute_reply":"2024-05-27T00:18:34.550508Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.1)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"id":"nnnf2LMYTfzN","outputId":"c02544af-edb0-4ffe-d48e-e1f2f4cdb15a","scrolled":true,"execution":{"iopub.status.busy":"2024-05-27T00:43:44.504718Z","iopub.execute_input":"2024-05-27T00:43:44.505079Z","iopub.status.idle":"2024-05-27T00:43:59.154242Z","shell.execute_reply.started":"2024-05-27T00:43:44.505044Z","shell.execute_reply":"2024-05-27T00:43:59.153084Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting ultralytics\n  Downloading ultralytics-8.2.22-py3-none-any.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: matplotlib>=3.3.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (3.7.5)\nRequirement already satisfied: opencv-python>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.9.0.80)\nRequirement already satisfied: pillow>=7.1.2 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.5.0)\nRequirement already satisfied: pyyaml>=5.3.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (6.0.1)\nRequirement already satisfied: requests>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.31.0)\nRequirement already satisfied: scipy>=1.4.1 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (1.11.4)\nRequirement already satisfied: torch>=1.8.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.2)\nRequirement already satisfied: torchvision>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.16.2)\nRequirement already satisfied: tqdm>=4.64.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (4.66.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from ultralytics) (5.9.3)\nRequirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from ultralytics) (9.0.0)\nCollecting thop>=0.1.1 (from ultralytics)\n  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: pandas>=1.1.4 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (2.1.4)\nRequirement already satisfied: seaborn>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from ultralytics) (0.12.2)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\nRequirement already satisfied: numpy<2,>=1.20 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (21.3)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2023.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2024.2.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\nDownloading ultralytics-8.2.22-py3-none-any.whl (778 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.4/778.4 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\nInstalling collected packages: thop, ultralytics\nSuccessfully installed thop-0.1.1.post2209072238 ultralytics-8.2.22\n","output_type":"stream"}]},{"cell_type":"code","source":"# https://drive.google.com/file/d/1BuMGx9QepfVSuyKE_Ux0CbnAhvSl_Oc5/view?usp=sharing\n#                                 |          this is the id       |\n# https://drive.google.com/uc?id=1BuMGx9QepfVSuyKE_Ux0CbnAhvSl_Oc5\n# extract ur id","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1BuMGx9QepfVSuyKE_Ux0CbnAhvSl_Oc5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://drive.google.com/file/d/1iCAFN2MIkngxldIvM940UA0InYtl6tJb/view?usp=drive_link\n# 1iCAFN2MIkngxldIvM940UA0InYtl6tJb","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!gdown --id 1iCAFN2MIkngxldIvM940UA0InYtl6tJb","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/image_processing/datasets/merge\n!ls /kaggle/working/image_processing/datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for extracting\n!unzip /kaggle/working/merge.zip -d /kaggle/working/image_processing/datasets","metadata":{"id":"Q8h0vYNoNWKr","outputId":"11a2f829-5d1d-466b-d9c3-2be2e96014b9","_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/merge.zip\n%cd /kaggle/working/image_processing\n!ls /kaggle/working/image_processing/datasets","metadata":{"id":"cQ1sk19NrhV3","outputId":"277e53df-50d0-4758-e8d2-6004d3d6919a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -r '/content/drive/MyDrive/image_processingyolov9/requirements.txt'\n# !pip install -r '/content/drive/MyDrive/image_processingMid-term Evalution code/requirements.txt'","metadata":{"id":"x_tTywVptW52","outputId":"e11e2c40-a720-4a89-edf0-b9cb1f6c86c3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prune + some minor stuffs","metadata":{}},{"cell_type":"markdown","source":"## Note\n`train`: v9\n\n`train5`: v8","metadata":{}},{"cell_type":"code","source":"# %rm -rf /kaggle/working/image_processing/runs/detect/train2\n%rm -rf /kaggle/working/image_processing/runs/detect/train3\n# %rm -rf /kaggle/working/image_processing/runs/detect/train4\n\n# %rm -rf /kaggle/working/image_processing/yolov9\n\n%rm -rf /kaggle/working/runs/\n\n%rm -rf /kaggle/working/image_processing.zip\n%rm -rf /kaggle/working/output.zip\n%rm -rf /kaggle/working/augment2.zip","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/working/","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!wandb off","metadata":{"execution":{"iopub.status.busy":"2024-05-22T19:56:01.451802Z","iopub.execute_input":"2024-05-22T19:56:01.452183Z","iopub.status.idle":"2024-05-22T19:56:03.539711Z","shell.execute_reply.started":"2024-05-22T19:56:01.452148Z","shell.execute_reply":"2024-05-22T19:56:03.538777Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# v8","metadata":{"id":"irlgl5eOtqD3"}},{"cell_type":"code","source":"%cd /kaggle/working/image_processing","metadata":{"execution":{"iopub.status.busy":"2024-05-22T19:56:07.095969Z","iopub.execute_input":"2024-05-22T19:56:07.096308Z","iopub.status.idle":"2024-05-22T19:56:07.103284Z","shell.execute_reply.started":"2024-05-22T19:56:07.096281Z","shell.execute_reply":"2024-05-22T19:56:07.102461Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/working/image_processing\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -rf /kaggle/working/image_processing/runs/detect/v8x_merge_aug12\n!rm -rf /kaggle/working/image_processing/runs/detect/v8x_merge_aug122\n!ls /kaggle/working/image_processing/runs/detect","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"id":"cjP5Ni0VUW4v","execution":{"iopub.status.busy":"2024-05-22T19:56:10.131314Z","iopub.execute_input":"2024-05-22T19:56:10.131692Z","iopub.status.idle":"2024-05-22T19:56:13.724754Z","shell.execute_reply.started":"2024-05-22T19:56:10.131659Z","shell.execute_reply":"2024-05-22T19:56:13.723952Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# weight_path = '/kaggle/working/image_processing/runs/detect/train2/weights/best.pt'\n# model = YOLO(weight_path)\n\nmodel = YOLO('yolov8x.pt')\n\nmodel","metadata":{"id":"uQpf5qqRUiX4","scrolled":true,"execution":{"iopub.status.busy":"2024-05-22T20:01:14.392070Z","iopub.execute_input":"2024-05-22T20:01:14.392809Z","iopub.status.idle":"2024-05-22T20:01:14.553285Z","shell.execute_reply.started":"2024-05-22T20:01:14.392776Z","shell.execute_reply":"2024-05-22T20:01:14.552380Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(800, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 80, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"dataset = '/kaggle/working/image_processing/datasets/merge/training.yaml'\n\n# results = model.train(data = dataset, batch = 4, epochs = 100, imgsz = 928, seed = 18022004, device = [0, 1], name = 'v8x_merge_aug12', resume = True)\n\nresults = model.train(data = dataset, batch = 4, epochs = 100, imgsz = 928, seed = 18022004, device = [0, 1], save_period = 5, name = 'v8x_merge_aug12')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-22T20:01:24.032011Z","iopub.execute_input":"2024-05-22T20:01:24.032784Z","iopub.status.idle":"2024-05-22T23:32:15.979319Z","shell.execute_reply.started":"2024-05-22T20:01:24.032746Z","shell.execute_reply":"2024-05-22T23:32:15.978246Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.19 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8x.pt, data=/kaggle/working/image_processing/datasets/merge/training.yaml, epochs=100, time=None, patience=100, batch=4, imgsz=928, save=True, save_period=5, cache=False, device=[0, 1], workers=8, project=None, name=v8x_merge_aug122, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=18022004, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/v8x_merge_aug122\nOverriding model.yaml nc=80 with nc=1\n\n                   from  n    params  module                                       arguments                     \n  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n  5                  -1  1   1844480  ultralytics.nn.modules.conv.Conv             [320, 640, 3, 2]              \n  6                  -1  6  13117440  ultralytics.nn.modules.block.C2f             [640, 640, 6, True]           \n  7                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n  8                  -1  3   6969600  ultralytics.nn.modules.block.C2f             [640, 640, 3, True]           \n  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 12                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 15                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n 16                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 18                  -1  3   7174400  ultralytics.nn.modules.block.C2f             [960, 640, 3]                 \n 19                  -1  1   3687680  ultralytics.nn.modules.conv.Conv             [640, 640, 3, 2]              \n 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n 21                  -1  3   7379200  ultralytics.nn.modules.block.C2f             [1280, 640, 3]                \n 22        [15, 18, 21]  1   8718931  ultralytics.nn.modules.head.Detect           [1, [320, 640, 640]]          \nModel summary: 365 layers, 68153571 parameters, 68153555 gradients, 258.1 GFLOPs\n\nTransferred 589/595 items from pretrained weights\n\u001b[34m\u001b[1mDDP:\u001b[0m debug command /opt/conda/bin/python3.10 -m torch.distributed.run --nproc_per_node 2 --master_port 44823 /root/.config/Ultralytics/DDP/_temp_hb0uzq5n138476083570592.py\nUltralytics YOLOv8.2.19 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/v8x_merge_aug122', view at http://localhost:6006/\n","output_type":"stream"},{"name":"stderr","text":"wandb: Tracking run with wandb version 0.16.6\nwandb: W&B syncing is set to `offline` in this directory.  \nwandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n","output_type":"stream"},{"name":"stdout","text":"Overriding model.yaml nc=80 with nc=1\nTransferred 589/595 items from pretrained weights\nFreezing layer 'model.22.dfl.conv.weight'\n\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mtrain: \u001b[0mScanning /kaggle/working/image_processing/datasets/merge/labels/train.cache... 630 images, 0 backgrounds, 0 corrupt: 100%|██████████| 630/630 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/image_processing/datasets/merge/labels/val.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|██████████| 10/10 [00:00<?, ?it/s]\n","output_type":"stream"},{"name":"stdout","text":"Plotting labels to runs/detect/v8x_merge_aug122/labels.jpg... \n\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000714, momentum=0.9) with parameter groups 97 weight(decay=0.0), 104 weight(decay=0.0005), 103 bias(decay=0.0)\n\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\nImage sizes 928 train, 928 val\nUsing 4 dataloader workers\nLogging results to \u001b[1mruns/detect/v8x_merge_aug122\u001b[0m\nStarting training for 100 epochs...\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      1/100      6.81G      1.105      0.726     0.9701        216        928: 100%|██████████| 158/158 [02:39<00:00,  1.01s/it]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:01<00:00,  2.78it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.996      0.988      0.995      0.819\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      2/100      7.59G      0.776      0.387     0.8563        243        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.996      0.991      0.995      0.839\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      3/100      6.78G     0.8024     0.3869     0.8663        273        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.992      0.995      0.845\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      4/100      6.86G     0.7459      0.388      0.851        171        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995       0.85\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      5/100      7.82G     0.7141     0.3669     0.8472        108        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.845\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      6/100      8.15G     0.7201     0.3509     0.8456        372        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.877\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      7/100       8.3G     0.6838     0.3352     0.8346        517        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.992      0.996      0.995      0.842\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      8/100       7.5G      0.655     0.3158     0.8349        348        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.995      0.995      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"      9/100      8.34G     0.6388     0.3125     0.8255        483        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     10/100       7.7G     0.6554     0.3118     0.8282        194        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     11/100      7.89G       0.65     0.3074     0.8334        173        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.871\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     12/100       7.4G     0.6211     0.3004     0.8238        131        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995       0.88\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     13/100      8.37G     0.6115       0.29     0.8244        246        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995      0.872\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     14/100      6.91G     0.5781     0.2799     0.8197        291        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.995      0.995      0.878\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     15/100      6.62G     0.6064     0.2853     0.8209        178        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     16/100      6.89G      0.585     0.2744     0.8129        239        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.838\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     17/100      6.28G      0.562     0.2642     0.8158        509        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995       0.89\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     18/100      7.43G     0.5633     0.2651     0.8128        266        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.997      0.995      0.887\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     19/100      7.71G     0.5526     0.2637     0.8134        510        928: 100%|██████████| 158/158 [02:24<00:00,  1.09it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     20/100      7.09G     0.5446     0.2588       0.81        384        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.996      0.994      0.995      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     21/100      7.22G     0.5525     0.2601     0.8124        110        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.995      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     22/100      6.65G     0.5372     0.2598     0.8045        159        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     23/100      7.12G     0.5592     0.2655     0.8072         82        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.01it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.995      0.995      0.894\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     24/100      6.89G      0.539      0.254     0.8093        216        928: 100%|██████████| 158/158 [02:22<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.863\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     25/100      8.11G     0.5362     0.2536     0.8057         78        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     26/100      6.77G     0.5161     0.2494     0.8029        281        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     27/100      7.79G     0.5531     0.2586      0.811        238        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     28/100      7.28G     0.5339      0.256     0.8094        579        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.06it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     29/100      7.47G     0.4912     0.2394     0.8015        202        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     30/100      6.95G     0.5165     0.2425     0.8061        287        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.877\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     31/100      7.06G     0.5038     0.2387     0.8044        386        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     32/100      6.98G     0.4954      0.238     0.8007        350        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995       0.89\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     33/100      8.39G     0.4825     0.2338     0.7997        116        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     34/100      7.15G     0.4737     0.2309      0.801        296        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     35/100      7.29G     0.4659     0.2281      0.798        187        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     36/100      7.47G      0.469     0.2291     0.8011        222        928: 100%|██████████| 158/158 [02:22<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.994      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     37/100      7.05G     0.4462     0.2234     0.7948        192        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.11it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.994      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     38/100      8.69G     0.4381     0.2178     0.7932        147        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.996      0.996      0.995       0.89\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     39/100      7.76G     0.4537      0.222     0.7973        324        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     40/100      7.16G     0.4391     0.2179     0.7918        395        928: 100%|██████████| 158/158 [02:22<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.873\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     41/100      7.44G       0.43     0.2165     0.7913        445        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     42/100      6.94G     0.4271     0.2137     0.7887        151        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.15it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     43/100      7.66G     0.4168     0.2082      0.791        674        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.998      0.995      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     44/100      7.18G     0.4013     0.2017      0.792        204        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.874\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     45/100      7.11G     0.3995     0.2034     0.7868        254        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.887\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     46/100      7.59G     0.4301     0.2147     0.7913        608        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995       0.88\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     47/100      6.16G     0.4101     0.2081     0.7903        778        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.997      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     48/100      6.33G     0.4064     0.2049     0.7879        174        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     49/100      7.17G     0.3969     0.2034     0.7869        219        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.876\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     50/100       7.6G     0.3703     0.1928     0.7871         83        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     51/100      8.24G     0.3701     0.1916      0.786        225        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     52/100      6.96G     0.3881     0.1988     0.7855        450        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.992      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     53/100       7.6G     0.3753     0.1946     0.7883        382        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     54/100      6.72G     0.3856     0.1972     0.7887        193        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.99it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995       0.88\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     55/100      6.41G     0.3709     0.1903     0.7848        169        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     56/100      6.57G     0.3455     0.1824      0.783        384        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     57/100      7.53G     0.3507     0.1831     0.7822        248        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     58/100       6.3G     0.3566     0.1873     0.7812        132        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995       0.88\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     59/100      7.39G     0.3475     0.1823      0.784        512        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.97it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.995      0.995      0.887\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     60/100      8.41G     0.3457     0.1815     0.7798        291        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     61/100      7.07G     0.3464     0.1814     0.7822        496        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.997      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     62/100      7.93G     0.3545     0.1841     0.7846        313        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     63/100       7.1G     0.3509      0.184     0.7827        673        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.995      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     64/100      7.24G     0.3309     0.1764     0.7818        155        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.997      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     65/100      5.71G     0.3321     0.1748     0.7798        679        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     66/100      6.47G     0.3223     0.1718     0.7789        558        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  4.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     67/100      8.39G     0.3272     0.1727      0.783        564        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.14it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.998      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     68/100      7.35G     0.3278      0.172     0.7793        267        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     69/100      6.86G       0.31     0.1686     0.7764        190        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.993      0.996      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     70/100      7.25G     0.3182     0.1713     0.7792         94        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     71/100      7.44G     0.3206     0.1716     0.7785        746        928: 100%|██████████| 158/158 [02:22<00:00,  1.11it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.998      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     72/100      7.95G     0.3203     0.1723     0.7763        309        928: 100%|██████████| 158/158 [02:23<00:00,  1.10it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.05it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.998      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     73/100      7.24G     0.3071     0.1652     0.7773        528        928:  54%|█████▍    | 86/158 [01:17<01:03,  1.13it/s][E ProcessGroupNCCL.cpp:475] [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4216, OpType=BROADCAST, NumelIn=58080, NumelOut=58080, Timeout(ms)=10800000) ran for 10800000 milliseconds before timing out.\n     73/100      7.24G     0.3063      0.165     0.7769        545        928:  58%|█████▊    | 91/158 [01:22<01:00,  1.11it/s][E ProcessGroupNCCL.cpp:489] Some NCCL operations have failed or timed out. Due to the asynchronous nature of CUDA kernels, subsequent GPU operations might run on corrupted/incomplete data.\n[E ProcessGroupNCCL.cpp:495] To avoid data inconsistency, we are taking the entire process down.\n[E ProcessGroupNCCL.cpp:916] [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4216, OpType=BROADCAST, NumelIn=58080, NumelOut=58080, Timeout(ms)=10800000) ran for 10800000 milliseconds before timing out.\nterminate called after throwing an instance of 'std::runtime_error'\n  what():  [Rank 1] NCCL watchdog thread terminated with exception: [Rank 1] Watchdog caught collective operation timeout: WorkNCCL(SeqNum=4216, OpType=BROADCAST, NumelIn=58080, NumelOut=58080, Timeout(ms)=10800000) ran for 10800000 milliseconds before timing out.\n     73/100      7.25G     0.3086     0.1643     0.7791        282        928: 100%|██████████| 158/158 [01:50<00:00,  1.43it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     74/100      6.46G     0.2951     0.1595     0.7751        293        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     75/100      6.27G     0.2979     0.1596     0.7767        327        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     76/100      6.59G     0.2989     0.1614     0.7753        646        928: 100%|██████████| 158/158 [01:07<00:00,  2.34it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.889\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     77/100      6.94G     0.2917     0.1573     0.7773        475        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.886\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     78/100      6.48G     0.2981     0.1606      0.776        755        928: 100%|██████████| 158/158 [01:06<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     79/100      7.07G      0.292     0.1584     0.7754        186        928: 100%|██████████| 158/158 [01:06<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     80/100      6.17G     0.2991     0.1606     0.7765        225        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.883\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     81/100      6.16G     0.2858     0.1537     0.7757        719        928: 100%|██████████| 158/158 [01:06<00:00,  2.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     82/100      7.86G       0.29     0.1575     0.7724        215        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.881\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     83/100      6.86G     0.2745     0.1497     0.7737         88        928: 100%|██████████| 158/158 [01:06<00:00,  2.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.887\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     84/100      8.88G     0.2796     0.1525     0.7714         80        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.882\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     85/100      7.11G     0.2729     0.1479     0.7764        620        928: 100%|██████████| 158/158 [01:06<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     86/100      7.46G      0.281     0.1514     0.7737         96        928: 100%|██████████| 158/158 [01:06<00:00,  2.37it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.888\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     87/100      8.07G     0.2791     0.1512     0.7753        563        928: 100%|██████████| 158/158 [01:07<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995       0.88\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     88/100      7.31G      0.281     0.1522     0.7743        681        928: 100%|██████████| 158/158 [01:07<00:00,  2.35it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.884\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     89/100      6.72G     0.2734     0.1481     0.7719        346        928: 100%|██████████| 158/158 [01:06<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.885\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     90/100      7.82G     0.2689     0.1455     0.7728        333        928: 100%|██████████| 158/158 [01:07<00:00,  2.36it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.996      0.995      0.882\nClosing dataloader mosaic\n\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     91/100      5.52G     0.3166     0.1629     0.7763        131        928: 100%|██████████| 158/158 [01:09<00:00,  2.26it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     92/100      5.36G     0.3007     0.1564     0.7744        131        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     93/100      5.68G     0.2732     0.1461      0.769        124        928: 100%|██████████| 158/158 [01:04<00:00,  2.47it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     94/100       5.2G     0.2646     0.1422     0.7716        254        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.891\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     95/100      5.45G     0.2597     0.1418     0.7691        176        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     96/100      5.22G     0.2529     0.1374     0.7682        236        928: 100%|██████████| 158/158 [01:04<00:00,  2.47it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.996      0.995      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     97/100       5.7G     0.2518     0.1372      0.769        142        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.994      0.997      0.995      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     98/100      5.36G     0.2504     0.1354     0.7689         89        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.892\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"     99/100      5.49G     0.2495     0.1362     0.7703        227        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.893\n\n      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n","output_type":"stream"},{"name":"stderr","text":"    100/100       5.5G      0.234      0.129      0.768        147        928: 100%|██████████| 158/158 [01:04<00:00,  2.46it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:00<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.892\n\n100 epochs completed in 3.490 hours.\nOptimizer stripped from runs/detect/v8x_merge_aug122/weights/last.pt, 136.8MB\nOptimizer stripped from runs/detect/v8x_merge_aug122/weights/best.pt, 136.8MB\n\nValidating runs/detect/v8x_merge_aug122/weights/best.pt...\nUltralytics YOLOv8.2.19 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\nModel summary (fused): 268 layers, 68124531 parameters, 0 gradients, 257.4 GFLOPs\n","output_type":"stream"},{"name":"stderr","text":"                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 3/3 [00:13<00:00,  4.56s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.995      0.995      0.894\nSpeed: 0.7ms preprocess, 46.7ms inference, 0.0ms loss, 1.7ms postprocess per image\nResults saved to \u001b[1mruns/detect/v8x_merge_aug122\u001b[0m\n","output_type":"stream"},{"name":"stderr","text":"wandb:                                                                                \nwandb: \nwandb: Run history:\nwandb:                  lr/pg0 ▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\nwandb:                  lr/pg1 ▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\nwandb:                  lr/pg2 ▃███▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\nwandb:        metrics/mAP50(B) ▅▁▆▆▁▆▇▄█▆▆▇▆▇▇▇▇█▆█▇██▇██▇████▇███▇███▆\nwandb:     metrics/mAP50-95(B) ▁▃▆▇▆▆▃▇▇█▇▇▇█▇▇█▆▇▆▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇████\nwandb:    metrics/precision(B) █▁▅▃▅▁▄▂▃▆▃▇▄▅▅▆▄▆▄▅▆▆▅▆▂▂▆▃▅▄▆▅▄▄▆▅▆▆▅▆\nwandb:       metrics/recall(B) ▁▄▆▆▇▇▇▇▆��▇▇▇▇▅▇▆▇▆█▇▇▇▆▇▇█▆█▇█▇▇▇▇▇█▇▇▆\nwandb:            model/GFLOPs ▁\nwandb:        model/parameters ▁\nwandb: model/speed_PyTorch(ms) ▁\nwandb:          train/box_loss █▆▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▁▁▁\nwandb:          train/cls_loss █▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁\nwandb:          train/dfl_loss █▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁\nwandb:            val/box_loss █▆▂▂▄▂▅▁▁▁▂▂▂▁▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\nwandb:            val/cls_loss █▇▄▃▄▃▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\nwandb:            val/dfl_loss █▆▃▂▃▂▄▁▁▁▁▁▂▁▁▁▁▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂\nwandb: \nwandb: Run summary:\nwandb:                  lr/pg0 1e-05\nwandb:                  lr/pg1 1e-05\nwandb:                  lr/pg2 1e-05\nwandb:        metrics/mAP50(B) 0.9949\nwandb:     metrics/mAP50-95(B) 0.89413\nwandb:    metrics/precision(B) 0.99489\nwandb:       metrics/recall(B) 0.9954\nwandb:            model/GFLOPs 258.123\nwandb:        model/parameters 68153571\nwandb: model/speed_PyTorch(ms) 88.779\nwandb:          train/box_loss 0.234\nwandb:          train/cls_loss 0.12895\nwandb:          train/dfl_loss 0.768\nwandb:            val/box_loss 0.55258\nwandb:            val/cls_loss 0.2153\nwandb:            val/dfl_loss 0.81877\nwandb: \nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync /kaggle/working/image_processing/wandb/offline-run-20240522_200136-yylfolzb\nwandb: Find logs at: ./wandb/offline-run-20240522_200136-yylfolzb/logs\n","output_type":"stream"}]},{"cell_type":"code","source":"# !yolo train model = {weight_dir} \\\n#     data = {dataset} \\\n#     batch = 2 \\\n#     epochs = 100 \\\n#     imgsz = 928 \\\n#     seed = 18022004","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"id":"K3E815MeH5iU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v9","metadata":{"id":"S0LJBZtGtsq7"}},{"cell_type":"code","source":"%cd /kaggle/working/image_processing","metadata":{"id":"BmdMEiuWUnrc","outputId":"62f8c476-3a80-4a5f-bc6b-b1fda0f7b810","execution":{"iopub.status.busy":"2024-05-27T00:44:08.999873Z","iopub.execute_input":"2024-05-27T00:44:09.000273Z","iopub.status.idle":"2024-05-27T00:44:09.007234Z","shell.execute_reply.started":"2024-05-27T00:44:09.000238Z","shell.execute_reply":"2024-05-27T00:44:09.006217Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/working/image_processing\n","output_type":"stream"}]},{"cell_type":"code","source":"# !python 'yolov9/train_dual.py' --workers 8 --device cpu --batch 16 \\\n#  --data 'wb_localization_dataset/training_v9.yaml' --img 900 --cfg 'yolov9/models/detect/yolov9-e.yaml' \\\n#  --weights 'yolov9/yolov9-e.pt' --name 'yolov9-e' --hyp 'yolov9/data/hyps/hyp.scratch-high.yaml' \\\n#  --min-items 0 --epochs 500 --close-mosaic 15","metadata":{"id":"KtOrNYsNtu2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"id":"BO82lNAY1hsr","execution":{"iopub.status.busy":"2024-05-27T00:19:20.044585Z","iopub.execute_input":"2024-05-27T00:19:20.045501Z","iopub.status.idle":"2024-05-27T00:19:24.349370Z","shell.execute_reply.started":"2024-05-27T00:19:20.045458Z","shell.execute_reply":"2024-05-27T00:19:24.348470Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"!wandb off","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:44:12.226390Z","iopub.execute_input":"2024-05-27T00:44:12.226742Z","iopub.status.idle":"2024-05-27T00:44:14.371183Z","shell.execute_reply.started":"2024-05-27T00:44:12.226713Z","shell.execute_reply":"2024-05-27T00:44:14.370086Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"W&B offline. Running your script from this directory will only write metadata locally. Use wandb disabled to completely turn off W&B.\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown --id 12F2qgvgITh6NLFIjFpzwhWAst7pVJ3F3","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:20:19.487947Z","iopub.execute_input":"2024-05-27T00:20:19.488875Z","iopub.status.idle":"2024-05-27T00:20:24.007523Z","shell.execute_reply.started":"2024-05-27T00:20:19.488841Z","shell.execute_reply":"2024-05-27T00:20:24.006341Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/opt/conda/lib/python3.10/site-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=12F2qgvgITh6NLFIjFpzwhWAst7pVJ3F3\nFrom (redirected): https://drive.google.com/uc?id=12F2qgvgITh6NLFIjFpzwhWAst7pVJ3F3&confirm=t&uuid=062a6a07-c1b5-4c3f-aa63-3a34fadb2cb0\nTo: /kaggle/working/image_processing/yolov9-e-converted.pt\n100%|████████████████████████████████████████| 117M/117M [00:02<00:00, 45.0MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model_path = '/kaggle/working/image_processing/yolov9-e-converted.pt'\n# model = YOLO('yolov9e.yaml')\n# model = YOLO(model_path)\n# model","metadata":{"id":"Y8knVYjA1lGD","outputId":"b24caed2-0827-42ad-b373-99ec789213d6","execution":{"iopub.status.busy":"2024-05-27T00:44:24.373956Z","iopub.execute_input":"2024-05-27T00:44:24.374355Z","iopub.status.idle":"2024-05-27T00:44:24.378537Z","shell.execute_reply.started":"2024-05-27T00:44:24.374323Z","shell.execute_reply":"2024-05-27T00:44:24.377603Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# dataset = '/content/drive/MyDrive/image_processing/datasets/wb_localization_dataset/training.yaml'\ndataset = '/kaggle/working/image_processing/datasets/wb_localization_dataset/training.yaml'\n# dataset = '/kaggle/working/image_processing/datasets/augment/training.yaml'\n# results = model.train(data = dataset, batch = 8, device = [0, 1], epochs = 100, imgsz = 928, seed = 18022004)","metadata":{"id":"kmoTi9qe490D","outputId":"26d44b2e-bea6-4966-af9a-162a4a359a21","execution":{"iopub.status.busy":"2024-05-27T00:44:19.265145Z","iopub.execute_input":"2024-05-27T00:44:19.265937Z","iopub.status.idle":"2024-05-27T00:44:19.270770Z","shell.execute_reply.started":"2024-05-27T00:44:19.265901Z","shell.execute_reply":"2024-05-27T00:44:19.269502Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!rm -rf /kaggle/working/image_processing/yolov9/runs/train/*","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:46:20.047381Z","iopub.execute_input":"2024-05-27T00:46:20.048061Z","iopub.status.idle":"2024-05-27T00:46:20.996267Z","shell.execute_reply.started":"2024-05-27T00:46:20.048008Z","shell.execute_reply":"2024-05-27T00:46:20.995123Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# !python -m torch.distributed.launch \\\n# !torchrun \\\n!python \\\n'/kaggle/working/image_processing/yolov9/train_dual.py' \\\n--workers 8 \\\n--device 0 \\\n--sync-bn \\\n--batch 2 \\\n--data '/kaggle/working/image_processing/datasets/wb_localization_dataset/training.yaml' \\\n--img 928 \\\n--cfg '/kaggle/working/image_processing/yolov9/models/detect/yolov9-e.yaml' \\\n--weights '/kaggle/working/image_processing/yolov9-e-converted.pt' \\\n--name 'yolov9-e' \\\n--hyp '/kaggle/working/image_processing/yolov9/data/hyps/hyp.scratch-high.yaml' \\\n--min-items 0 \\\n--epochs 100 \\\n--close-mosaic 15","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"id":"ul5uX1TmDThs","outputId":"5e3a1746-3bc8-4724-cce6-d876bb0f18d5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# v10","metadata":{}},{"cell_type":"code","source":"%cd /kaggle/working/image_processing","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:51:24.572398Z","iopub.execute_input":"2024-05-27T00:51:24.572881Z","iopub.status.idle":"2024-05-27T00:51:24.579001Z","shell.execute_reply.started":"2024-05-27T00:51:24.572850Z","shell.execute_reply":"2024-05-27T00:51:24.578098Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/working/image_processing\n","output_type":"stream"}]},{"cell_type":"code","source":"from ultralytics import YOLO\n!wandb off","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:51:06.261218Z","iopub.execute_input":"2024-05-27T00:51:06.262060Z","iopub.status.idle":"2024-05-27T00:51:08.427373Z","shell.execute_reply.started":"2024-05-27T00:51:06.262026Z","shell.execute_reply":"2024-05-27T00:51:08.426579Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# weight_path = '/kaggle/working/image_processing/runs/detect/train2/weights/best.pt'\n# model = YOLO(weight_path)\n\nmodel = YOLO('yolov10x.pt')\n\nmodel","metadata":{"execution":{"iopub.status.busy":"2024-05-27T00:53:18.790800Z","iopub.execute_input":"2024-05-27T00:53:18.791487Z","iopub.status.idle":"2024-05-27T00:53:18.985965Z","shell.execute_reply.started":"2024-05-27T00:53:18.791455Z","shell.execute_reply":"2024-05-27T00:53:18.984577Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# weight_path = '/kaggle/working/image_processing/runs/detect/train2/weights/best.pt'\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# model = YOLO(weight_path)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mYOLO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myolov10x.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/models/yolo/model.py:23\u001b[0m, in \u001b[0;36mYOLO.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m \u001b[38;5;241m=\u001b[39m new_instance\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# Continue with default YOLO initialization\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:152\u001b[0m, in \u001b[0;36mModel.__init__\u001b[0;34m(self, model, task, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(model, task\u001b[38;5;241m=\u001b[39mtask, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/engine/model.py:241\u001b[0m, in \u001b[0;36mModel._load\u001b[0;34m(self, weights, task)\u001b[0m\n\u001b[1;32m    238\u001b[0m weights \u001b[38;5;241m=\u001b[39m checks\u001b[38;5;241m.\u001b[39mcheck_model_file_from_stem(weights)  \u001b[38;5;66;03m# add suffix, i.e. yolov8n -> yolov8n.pt\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(weights)\u001b[38;5;241m.\u001b[39msuffix \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;241m=\u001b[39m \u001b[43mattempt_load_one_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverrides \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset_ckpt_args(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39margs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:806\u001b[0m, in \u001b[0;36mattempt_load_one_weight\u001b[0;34m(weight, device, inplace, fuse)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mattempt_load_one_weight\u001b[39m(weight, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fuse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    805\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a single model weights.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 806\u001b[0m     ckpt, weight \u001b[38;5;241m=\u001b[39m \u001b[43mtorch_safe_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# load ckpt\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     args \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mDEFAULT_CFG_DICT, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m, {}))}  \u001b[38;5;66;03m# combine model and default args, preferring model args\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     model \u001b[38;5;241m=\u001b[39m (ckpt\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mema\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m ckpt[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# FP32 model\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ultralytics/nn/tasks.py:732\u001b[0m, in \u001b[0;36mtorch_safe_load\u001b[0;34m(weight)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    725\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m temporary_modules(\n\u001b[1;32m    726\u001b[0m         {\n\u001b[1;32m    727\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.yolo.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124multralytics.utils\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    730\u001b[0m         }\n\u001b[1;32m    731\u001b[0m     ):  \u001b[38;5;66;03m# for legacy 8.0 Classify and Pose models\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m         ckpt \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# e.name is missing module name\u001b[39;00m\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:986\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    984\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    989\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    990\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    991\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:435\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 435\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:416\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'yolov10x.pt'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'yolov10x.pt'","output_type":"error"}]},{"cell_type":"code","source":"dataset = '/kaggle/working/image_processing/datasets/merge/training.yaml'\n\n# results = model.train(data = dataset, batch = 4, epochs = 100, imgsz = 928, seed = 18022004, device = [0, 1], name = 'v8x_merge_aug12', resume = True)\n\nresults = model.train(data = dataset, batch = 4, epochs = 100, imgsz = 928, seed = 18022004, device = [0, 1], save_period = 5, name = 'v8x_merge_aug12')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Eval","metadata":{"id":"TL89OJp16vnf"}},{"cell_type":"code","source":"from ultralytics import YOLO","metadata":{"id":"53C5dCpUiVTX","execution":{"iopub.status.busy":"2024-05-23T04:32:29.428011Z","iopub.execute_input":"2024-05-23T04:32:29.428367Z","iopub.status.idle":"2024-05-23T04:32:33.727387Z","shell.execute_reply.started":"2024-05-23T04:32:29.428338Z","shell.execute_reply":"2024-05-23T04:32:33.726355Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"weight_path = '/kaggle/working/image_processing/runs/detect/train2/weights/best.pt'\n# weight_path = '/kaggle/working/image_processing/runs/detect/v8x_merge_aug122/weights/best.pt'\n\n# v9\n# model = YOLO('yolov9e.yaml')\n# model = YOLO(weight_path)\n\n# v8\nmodel = YOLO(weight_path)\n\nmodel","metadata":{"id":"AhZYrA41Eq-H","outputId":"57584add-0946-48a8-df8d-c0967a08b2e8","scrolled":true,"execution":{"iopub.status.busy":"2024-05-23T04:32:35.028067Z","iopub.execute_input":"2024-05-23T04:32:35.028811Z","iopub.status.idle":"2024-05-23T04:32:35.353912Z","shell.execute_reply.started":"2024-05-23T04:32:35.028778Z","shell.execute_reply":"2024-05-23T04:32:35.352969Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(800, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"dataset = '/kaggle/working/image_processing/datasets/wb_localization_dataset/training.yaml'\nmodel.val(data = dataset, batch = 8, imgsz = 928, device = [0, 1])","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-05-23T04:32:39.230165Z","iopub.execute_input":"2024-05-23T04:32:39.231019Z","iopub.status.idle":"2024-05-23T04:33:16.169684Z","shell.execute_reply.started":"2024-05-23T04:32:39.230987Z","shell.execute_reply":"2024-05-23T04:33:16.168736Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Ultralytics YOLOv8.2.19 🚀 Python-3.10.13 torch-2.1.2 CUDA:0 (Tesla T4, 15102MiB)\n                                                      CUDA:1 (Tesla T4, 15102MiB)\nModel summary (fused): 268 layers, 68124531 parameters, 0 gradients, 257.4 GFLOPs\nDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 755k/755k [00:00<00:00, 14.7MB/s]\n\u001b[34m\u001b[1mval: \u001b[0mScanning /kaggle/working/image_processing/datasets/wb_localization_dataset/labels/val.cache... 10 images, 0 backgrounds, 0 corrupt: 100%|██████████| 10/10 [00:00<?, ?it/s]\n                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 2/2 [00:17<00:00,  8.89s/it]\n","output_type":"stream"},{"name":"stdout","text":"                   all         10       1956      0.995      0.997      0.995      0.901\nSpeed: 7.5ms preprocess, 258.7ms inference, 0.0ms loss, 376.4ms postprocess per image\nResults saved to \u001b[1mruns/detect/val2\u001b[0m\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ultralytics.utils.metrics.DetMetrics object with attributes:\n\nap_class_index: array([0])\nbox: ultralytics.utils.metrics.Metric object\nconfusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x7842f8588370>\ncurves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\ncurves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99793,     0.99742,     0.99742,\n             0.9959,      0.9959,      0.9959,      0.9959,      0.9954,      0.9954,      0.9954,      0.9949,     0.76977,     0.38488,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[     0.9901,      0.9901,     0.99107,     0.99159,     0.99178,     0.99235,     0.99272,     0.99286,     0.99293,       0.993,     0.99307,     0.99316,     0.99364,     0.99365,     0.99366,     0.99367,     0.99368,     0.99369,      0.9937,     0.99371,     0.99372,     0.99373,     0.99374,\n            0.99375,     0.99376,     0.99377,     0.99378,     0.99379,     0.99381,     0.99382,     0.99383,     0.99384,     0.99385,     0.99386,     0.99387,     0.99388,     0.99389,     0.99394,     0.99398,     0.99403,     0.99407,     0.99412,      0.9942,     0.99432,      0.9944,      0.9944,\n            0.99441,     0.99442,     0.99442,     0.99443,     0.99443,     0.99444,     0.99445,     0.99445,     0.99446,     0.99447,     0.99447,     0.99448,     0.99449,     0.99449,      0.9945,     0.99451,     0.99451,     0.99452,     0.99453,     0.99453,     0.99454,     0.99455,     0.99455,\n            0.99456,     0.99456,     0.99457,     0.99458,     0.99458,     0.99459,      0.9946,      0.9946,     0.99461,     0.99462,     0.99462,     0.99463,     0.99464,     0.99464,     0.99465,     0.99466,     0.99467,     0.99468,     0.99469,      0.9947,      0.9947,     0.99471,     0.99472,\n            0.99473,     0.99474,     0.99475,     0.99476,     0.99477,     0.99478,     0.99478,     0.99479,      0.9948,     0.99481,     0.99482,     0.99483,     0.99484,     0.99485,     0.99486,     0.99487,     0.99487,     0.99488,     0.99489,      0.9949,     0.99492,     0.99493,     0.99495,\n            0.99496,     0.99498,     0.99499,     0.99501,     0.99502,     0.99504,     0.99505,     0.99507,     0.99508,      0.9951,     0.99511,     0.99513,     0.99514,     0.99516,     0.99518,     0.99519,     0.99521,     0.99523,     0.99524,     0.99526,     0.99528,      0.9953,     0.99531,\n            0.99533,     0.99535,     0.99537,     0.99538,      0.9954,     0.99542,     0.99544,     0.99546,     0.99548,      0.9955,     0.99552,     0.99554,     0.99556,     0.99558,      0.9956,     0.99562,     0.99564,     0.99566,     0.99567,     0.99569,      0.9957,     0.99571,     0.99573,\n            0.99574,     0.99575,     0.99576,     0.99578,     0.99579,      0.9958,     0.99581,     0.99583,     0.99584,     0.99585,     0.99587,     0.99588,     0.99589,      0.9959,     0.99592,     0.99592,     0.99592,     0.99592,     0.99592,     0.99593,     0.99593,     0.99593,     0.99593,\n            0.99593,     0.99593,     0.99594,     0.99594,     0.99594,     0.99594,     0.99594,     0.99594,     0.99595,     0.99595,     0.99595,     0.99595,     0.99595,     0.99595,     0.99596,     0.99596,     0.99596,     0.99596,     0.99596,     0.99597,     0.99597,     0.99597,     0.99597,\n            0.99597,     0.99597,     0.99598,     0.99598,     0.99598,     0.99598,     0.99598,     0.99598,     0.99599,     0.99599,     0.99599,     0.99599,     0.99599,     0.99599,       0.996,       0.996,       0.996,       0.996,       0.996,       0.996,     0.99601,     0.99601,     0.99601,\n            0.99601,     0.99601,     0.99602,     0.99602,     0.99602,     0.99602,     0.99602,     0.99602,     0.99603,     0.99603,     0.99603,     0.99603,     0.99603,     0.99603,     0.99604,     0.99604,     0.99604,     0.99604,     0.99604,     0.99604,     0.99605,     0.99605,     0.99605,\n            0.99605,     0.99605,     0.99606,     0.99606,     0.99606,     0.99606,     0.99606,     0.99606,     0.99607,     0.99607,     0.99607,     0.99607,     0.99607,     0.99607,     0.99608,     0.99608,     0.99608,     0.99608,     0.99608,     0.99608,     0.99609,     0.99609,     0.99609,\n            0.99609,     0.99609,      0.9961,      0.9961,      0.9961,      0.9961,      0.9961,      0.9961,     0.99611,     0.99611,     0.99611,     0.99611,     0.99611,     0.99611,     0.99612,     0.99612,     0.99612,     0.99612,     0.99612,     0.99612,     0.99613,     0.99613,     0.99613,\n            0.99613,     0.99613,     0.99613,     0.99614,     0.99614,     0.99614,     0.99614,     0.99614,     0.99615,     0.99615,     0.99615,     0.99615,     0.99615,     0.99615,     0.99616,     0.99616,     0.99616,     0.99616,     0.99616,     0.99616,     0.99617,     0.99617,     0.99617,\n            0.99617,     0.99616,     0.99616,     0.99615,     0.99614,     0.99614,     0.99613,     0.99612,     0.99612,     0.99611,     0.99611,      0.9961,     0.99609,     0.99609,     0.99608,     0.99608,     0.99607,     0.99606,     0.99606,     0.99605,     0.99605,     0.99604,     0.99603,\n            0.99603,     0.99602,     0.99601,     0.99601,       0.996,       0.996,     0.99599,     0.99598,     0.99598,     0.99597,     0.99597,     0.99596,     0.99595,     0.99595,     0.99594,     0.99593,     0.99593,     0.99592,     0.99592,     0.99591,     0.99591,     0.99591,      0.9959,\n             0.9959,      0.9959,     0.99589,     0.99589,     0.99589,     0.99588,     0.99588,     0.99588,     0.99588,     0.99587,     0.99587,     0.99587,     0.99586,     0.99586,     0.99586,     0.99585,     0.99585,     0.99585,     0.99584,     0.99584,     0.99584,     0.99584,     0.99583,\n            0.99583,     0.99583,     0.99582,     0.99582,     0.99582,     0.99581,     0.99581,     0.99581,      0.9958,      0.9958,      0.9958,      0.9958,     0.99579,     0.99579,     0.99579,     0.99578,     0.99578,     0.99578,     0.99577,     0.99577,     0.99577,     0.99576,     0.99576,\n            0.99576,     0.99576,     0.99575,     0.99575,     0.99575,     0.99574,     0.99574,     0.99574,     0.99573,     0.99573,     0.99573,     0.99573,     0.99572,     0.99572,     0.99572,     0.99571,     0.99571,     0.99571,      0.9957,      0.9957,      0.9957,     0.99569,     0.99569,\n            0.99569,     0.99569,     0.99568,     0.99568,     0.99568,     0.99567,     0.99567,     0.99567,     0.99566,     0.99566,     0.99566,     0.99566,     0.99566,     0.99566,     0.99566,     0.99567,     0.99567,     0.99567,     0.99567,     0.99567,     0.99567,     0.99568,     0.99568,\n            0.99568,     0.99568,     0.99568,     0.99568,     0.99569,     0.99569,     0.99569,     0.99569,     0.99569,      0.9957,      0.9957,      0.9957,      0.9957,      0.9957,      0.9957,     0.99571,     0.99571,     0.99571,     0.99571,     0.99571,     0.99571,     0.99572,     0.99572,\n            0.99572,     0.99572,     0.99572,     0.99572,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99573,     0.99574,     0.99574,     0.99574,     0.99574,     0.99574,     0.99574,     0.99575,     0.99575,     0.99575,     0.99575,     0.99575,     0.99575,     0.99576,\n            0.99576,     0.99576,     0.99576,     0.99576,     0.99577,     0.99577,     0.99577,     0.99577,     0.99577,     0.99577,     0.99578,     0.99578,     0.99578,     0.99578,     0.99578,     0.99578,     0.99579,     0.99579,     0.99579,     0.99579,     0.99579,     0.99579,      0.9958,\n             0.9958,      0.9958,      0.9958,      0.9958,      0.9958,     0.99581,     0.99581,     0.99581,     0.99581,     0.99581,     0.99581,     0.99582,     0.99582,     0.99582,     0.99582,     0.99582,     0.99582,     0.99583,     0.99583,     0.99583,     0.99583,     0.99583,     0.99583,\n            0.99584,     0.99584,     0.99584,     0.99584,     0.99584,     0.99584,     0.99585,     0.99585,     0.99585,     0.99585,     0.99585,     0.99586,     0.99586,     0.99586,     0.99586,     0.99586,     0.99586,     0.99587,     0.99587,     0.99587,     0.99587,     0.99587,     0.99587,\n            0.99588,     0.99588,     0.99588,     0.99588,     0.99588,     0.99588,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,     0.99591,     0.99591,     0.99591,     0.99591,      0.9959,\n            0.99589,     0.99587,     0.99586,     0.99585,     0.99583,     0.99582,      0.9958,     0.99579,     0.99577,     0.99576,     0.99574,     0.99573,     0.99571,      0.9957,     0.99568,     0.99567,     0.99565,     0.99563,      0.9956,     0.99558,     0.99555,     0.99553,      0.9955,\n            0.99548,     0.99545,     0.99543,      0.9954,      0.9954,     0.99539,     0.99539,     0.99538,     0.99538,     0.99537,     0.99537,     0.99537,     0.99536,     0.99536,     0.99535,     0.99535,     0.99535,     0.99534,     0.99534,     0.99533,     0.99533,     0.99533,     0.99532,\n            0.99532,     0.99531,     0.99531,      0.9953,      0.9953,      0.9953,     0.99529,     0.99529,     0.99528,     0.99528,     0.99528,     0.99527,     0.99527,     0.99526,     0.99526,     0.99526,     0.99525,     0.99525,     0.99524,     0.99524,     0.99523,     0.99523,     0.99523,\n            0.99522,     0.99522,     0.99521,     0.99521,     0.99521,      0.9952,      0.9952,     0.99519,     0.99519,     0.99519,     0.99518,     0.99518,     0.99517,     0.99517,     0.99516,     0.99516,     0.99516,     0.99515,     0.99515,     0.99514,     0.99513,      0.9951,     0.99508,\n            0.99505,     0.99503,       0.995,     0.99497,     0.99495,     0.99492,      0.9949,     0.99488,     0.99487,     0.99486,     0.99485,     0.99483,     0.99482,     0.99481,      0.9948,     0.99479,     0.99478,     0.99477,     0.99476,     0.99475,     0.99473,     0.99472,     0.99471,\n             0.9947,     0.99469,     0.99468,     0.99467,     0.99466,     0.99465,     0.99463,     0.99463,     0.99464,     0.99464,     0.99465,     0.99466,     0.99467,     0.99467,     0.99468,     0.99469,     0.99469,      0.9947,     0.99471,     0.99472,     0.99472,     0.99473,     0.99474,\n            0.99474,     0.99475,     0.99476,     0.99477,     0.99477,     0.99478,     0.99479,     0.99479,      0.9948,     0.99481,     0.99482,     0.99482,     0.99483,     0.99484,     0.99484,     0.99485,     0.99486,     0.99487,     0.99487,     0.99488,     0.99487,     0.99486,     0.99484,\n            0.99482,     0.99481,     0.99479,     0.99478,     0.99476,     0.99474,     0.99473,     0.99471,      0.9947,     0.99468,     0.99466,     0.99465,     0.99463,     0.99462,      0.9946,     0.99459,     0.99458,     0.99456,     0.99455,     0.99454,     0.99452,     0.99451,      0.9945,\n            0.99448,     0.99447,     0.99446,     0.99444,     0.99443,     0.99442,      0.9944,     0.99439,     0.99438,     0.99435,     0.99427,     0.99419,     0.99411,     0.99394,     0.99384,     0.99382,      0.9938,     0.99378,     0.99376,     0.99374,     0.99372,      0.9937,     0.99368,\n            0.99366,     0.99364,     0.99362,      0.9936,     0.99354,     0.99346,     0.99338,     0.99311,     0.99303,     0.99297,     0.99292,     0.99287,     0.99295,     0.99305,     0.99302,     0.99299,     0.99296,     0.99293,      0.9929,     0.99288,     0.99285,     0.99282,     0.99288,\n            0.99295,     0.99301,     0.99308,     0.99317,     0.99326,     0.99331,     0.99327,     0.99322,     0.99318,     0.99313,     0.99309,     0.99283,     0.99258,     0.99265,     0.99273,      0.9928,     0.99257,     0.99248,      0.9924,     0.99233,     0.99246,     0.99251,     0.99247,\n            0.99243,     0.99238,     0.99234,      0.9923,     0.99225,      0.9922,     0.99216,     0.99211,     0.99206,     0.99198,      0.9918,     0.99164,     0.99146,     0.99119,     0.99109,     0.99099,     0.99085,      0.9907,     0.99056,     0.99034,     0.99019,     0.99016,     0.99013,\n             0.9901,     0.99007,     0.99004,     0.99002,     0.98999,     0.98996,      0.9899,     0.98981,     0.98973,     0.98957,     0.98933,     0.98913,     0.98867,     0.98891,     0.98899,     0.98907,     0.98915,       0.989,      0.9886,     0.98823,     0.98794,      0.9878,     0.98771,\n            0.98762,     0.98714,      0.9857,     0.98516,     0.98445,     0.98371,     0.98293,     0.98158,     0.98058,     0.97942,     0.97839,     0.97594,     0.97482,     0.97242,     0.97096,     0.96853,     0.96654,     0.96286,      0.9612,     0.95938,     0.95637,     0.95434,        0.95,\n            0.94571,     0.94201,     0.93848,     0.93582,     0.92926,     0.92412,      0.9185,      0.9107,     0.90195,     0.88726,     0.87216,     0.85208,     0.83219,      0.8054,     0.77052,      0.7275,     0.68232,     0.63683,     0.57273,     0.50208,     0.42451,     0.33845,     0.26096,\n            0.18896,     0.13487,    0.080701,    0.053465,    0.031934,    0.011686,    0.005152,   0.0023098,   0.0017213,   0.0013067,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.98287,     0.98287,     0.98478,     0.98581,     0.98618,     0.98732,     0.98805,     0.98831,     0.98846,     0.98859,     0.98872,     0.98892,     0.98986,     0.98988,     0.98991,     0.98993,     0.98995,     0.98997,     0.98999,     0.99001,     0.99003,     0.99005,     0.99007,\n            0.99009,     0.99011,     0.99013,     0.99015,     0.99017,     0.99019,     0.99021,     0.99023,     0.99025,     0.99027,      0.9903,     0.99032,     0.99034,     0.99036,     0.99045,     0.99054,     0.99064,     0.99073,     0.99082,     0.99099,     0.99121,     0.99137,     0.99138,\n            0.99139,     0.99141,     0.99142,     0.99143,     0.99144,     0.99146,     0.99147,     0.99148,      0.9915,     0.99151,     0.99152,     0.99153,     0.99155,     0.99156,     0.99157,     0.99159,      0.9916,     0.99161,     0.99162,     0.99164,     0.99165,     0.99166,     0.99168,\n            0.99169,      0.9917,     0.99172,     0.99173,     0.99174,     0.99175,     0.99177,     0.99178,     0.99179,     0.99181,     0.99182,     0.99183,     0.99184,     0.99186,     0.99187,     0.99189,     0.99191,     0.99193,     0.99194,     0.99196,     0.99198,       0.992,     0.99201,\n            0.99203,     0.99205,     0.99207,     0.99209,      0.9921,     0.99212,     0.99214,     0.99216,     0.99218,     0.99219,     0.99221,     0.99223,     0.99225,     0.99226,     0.99228,      0.9923,     0.99232,     0.99234,     0.99235,     0.99237,      0.9924,     0.99243,     0.99246,\n            0.99249,     0.99252,     0.99255,     0.99258,     0.99261,     0.99264,     0.99267,      0.9927,     0.99273,     0.99276,     0.99279,     0.99282,     0.99285,     0.99288,     0.99292,     0.99295,     0.99299,     0.99302,     0.99306,     0.99309,     0.99312,     0.99316,     0.99319,\n            0.99323,     0.99326,      0.9933,     0.99333,     0.99337,      0.9934,     0.99344,     0.99348,     0.99352,     0.99356,      0.9936,     0.99364,     0.99368,     0.99372,     0.99376,      0.9938,     0.99384,     0.99388,     0.99391,     0.99394,     0.99396,     0.99399,     0.99401,\n            0.99404,     0.99406,     0.99409,     0.99411,     0.99414,     0.99417,     0.99419,     0.99422,     0.99424,     0.99427,     0.99429,     0.99432,     0.99434,     0.99437,     0.99439,      0.9944,      0.9944,      0.9944,     0.99441,     0.99441,     0.99441,     0.99442,     0.99442,\n            0.99442,     0.99443,     0.99443,     0.99444,     0.99444,     0.99444,     0.99445,     0.99445,     0.99445,     0.99446,     0.99446,     0.99446,     0.99447,     0.99447,     0.99447,     0.99448,     0.99448,     0.99448,     0.99449,     0.99449,     0.99449,      0.9945,      0.9945,\n             0.9945,     0.99451,     0.99451,     0.99451,     0.99452,     0.99452,     0.99453,     0.99453,     0.99453,     0.99454,     0.99454,     0.99454,     0.99455,     0.99455,     0.99455,     0.99456,     0.99456,     0.99456,     0.99457,     0.99457,     0.99457,     0.99458,     0.99458,\n            0.99458,     0.99459,     0.99459,     0.99459,      0.9946,      0.9946,      0.9946,     0.99461,     0.99461,     0.99462,     0.99462,     0.99462,     0.99463,     0.99463,     0.99463,     0.99464,     0.99464,     0.99464,     0.99465,     0.99465,     0.99465,     0.99466,     0.99466,\n            0.99466,     0.99467,     0.99467,     0.99467,     0.99468,     0.99468,     0.99468,     0.99469,     0.99469,     0.99469,      0.9947,      0.9947,      0.9947,     0.99471,     0.99471,     0.99472,     0.99472,     0.99472,     0.99473,     0.99473,     0.99473,     0.99474,     0.99474,\n            0.99474,     0.99475,     0.99475,     0.99475,     0.99476,     0.99476,     0.99476,     0.99477,     0.99477,     0.99477,     0.99478,     0.99478,     0.99478,     0.99479,     0.99479,     0.99479,      0.9948,      0.9948,     0.99481,     0.99481,     0.99481,     0.99482,     0.99482,\n            0.99482,     0.99483,     0.99483,     0.99483,     0.99484,     0.99484,     0.99484,     0.99485,     0.99485,     0.99485,     0.99486,     0.99486,     0.99486,     0.99487,     0.99487,     0.99487,     0.99488,     0.99488,     0.99488,     0.99489,     0.99489,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,\n             0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,      0.9949,     0.99491,     0.99491,     0.99491,     0.99492,     0.99492,     0.99492,     0.99493,     0.99493,     0.99493,     0.99494,\n            0.99494,     0.99494,     0.99495,     0.99495,     0.99495,     0.99496,     0.99496,     0.99496,     0.99497,     0.99497,     0.99497,     0.99498,     0.99498,     0.99498,     0.99499,     0.99499,     0.99499,       0.995,       0.995,       0.995,     0.99501,     0.99501,     0.99501,\n            0.99502,     0.99502,     0.99502,     0.99503,     0.99503,     0.99503,     0.99504,     0.99504,     0.99505,     0.99505,     0.99505,     0.99506,     0.99506,     0.99506,     0.99507,     0.99507,     0.99507,     0.99508,     0.99508,     0.99508,     0.99509,     0.99509,     0.99509,\n             0.9951,      0.9951,      0.9951,     0.99511,     0.99511,     0.99511,     0.99512,     0.99512,     0.99512,     0.99513,     0.99513,     0.99513,     0.99514,     0.99514,     0.99514,     0.99515,     0.99515,     0.99515,     0.99516,     0.99516,     0.99516,     0.99517,     0.99517,\n            0.99517,     0.99518,     0.99518,     0.99518,     0.99519,     0.99519,     0.99519,      0.9952,      0.9952,      0.9952,     0.99521,     0.99521,     0.99521,     0.99522,     0.99522,     0.99523,     0.99523,     0.99523,     0.99524,     0.99524,     0.99524,     0.99525,     0.99525,\n            0.99525,     0.99526,     0.99526,     0.99526,     0.99527,     0.99527,     0.99527,     0.99528,     0.99528,     0.99528,     0.99529,     0.99529,     0.99529,      0.9953,      0.9953,      0.9953,     0.99531,     0.99531,     0.99531,     0.99532,     0.99532,     0.99532,     0.99533,\n            0.99533,     0.99533,     0.99534,     0.99534,     0.99534,     0.99535,     0.99535,     0.99535,     0.99536,     0.99536,     0.99536,     0.99537,     0.99537,     0.99537,     0.99538,     0.99538,     0.99538,     0.99539,     0.99539,      0.9954,      0.9954,      0.9954,      0.9954,\n             0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,\n             0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,\n             0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,\n             0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,      0.9954,\n             0.9954,      0.9954,      0.9954,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,\n            0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,     0.99539,      0.9954,     0.99541,     0.99543,     0.99544,     0.99545,     0.99547,     0.99548,      0.9955,     0.99551,     0.99553,     0.99554,     0.99555,     0.99557,     0.99558,      0.9956,     0.99561,\n            0.99563,     0.99564,     0.99565,     0.99567,     0.99568,      0.9957,     0.99571,     0.99572,     0.99574,     0.99575,     0.99577,     0.99578,      0.9958,     0.99581,     0.99582,     0.99584,     0.99585,     0.99587,     0.99588,      0.9959,      0.9959,      0.9959,      0.9959,\n             0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,\n             0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,      0.9959,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,\n            0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99589,     0.99616,      0.9964,      0.9964,      0.9964,      0.9964,      0.9964,      0.9964,      0.9964,      0.9964,      0.9964,     0.99653,\n            0.99666,     0.99679,     0.99693,     0.99711,     0.99729,     0.99742,     0.99742,     0.99742,     0.99742,     0.99742,     0.99742,     0.99742,     0.99748,     0.99763,     0.99778,     0.99792,     0.99793,     0.99793,     0.99793,     0.99793,     0.99828,     0.99845,     0.99845,\n            0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99845,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,\n            0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,     0.99844,      0.9985,     0.99899,     0.99915,     0.99932,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,     0.99948,\n            0.99948,     0.99948,     0.99947,     0.99947,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1,\n                  1,           1,           1,           1,           1,           1,           1,           1,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,     0.99744,\n            0.99744,     0.99743,     0.99741,      0.9974,     0.99739,     0.99738,     0.99737,     0.99735,     0.99734,     0.99733,     0.99732,      0.9973,     0.99729,     0.99728,     0.99727,     0.99725,     0.99724,     0.99723,     0.99722,     0.99721,     0.99719,     0.99718,     0.99717,\n            0.99716,     0.99714,     0.99713,     0.99712,     0.99711,      0.9971,     0.99708,     0.99707,     0.99706,     0.99705,     0.99703,     0.99702,     0.99701,       0.997,     0.99699,     0.99697,     0.99696,     0.99695,     0.99694,     0.99693,     0.99692,     0.99692,     0.99691,\n             0.9969,      0.9969,     0.99689,     0.99689,     0.99688,     0.99687,     0.99687,     0.99686,     0.99685,     0.99685,     0.99684,     0.99684,     0.99683,     0.99682,     0.99682,     0.99681,     0.99681,      0.9968,     0.99679,     0.99679,     0.99678,     0.99678,     0.99677,\n            0.99676,     0.99676,     0.99675,     0.99674,     0.99674,     0.99673,     0.99673,     0.99672,     0.99671,     0.99671,      0.9967,      0.9967,     0.99669,     0.99668,     0.99668,     0.99667,     0.99667,     0.99666,     0.99665,     0.99665,     0.99664,     0.99664,     0.99663,\n            0.99662,     0.99662,     0.99661,      0.9966,      0.9966,     0.99659,     0.99659,     0.99658,     0.99657,     0.99657,     0.99656,     0.99656,     0.99655,     0.99654,     0.99654,     0.99653,     0.99653,     0.99652,     0.99651,     0.99651,      0.9965,     0.99649,     0.99649,\n            0.99648,     0.99648,     0.99647,     0.99646,     0.99646,     0.99645,     0.99645,     0.99644,     0.99643,     0.99643,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,\n            0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,     0.99642,      0.9964,\n            0.99638,     0.99635,     0.99632,     0.99629,     0.99626,     0.99623,      0.9962,     0.99617,     0.99614,     0.99611,     0.99608,     0.99605,     0.99603,       0.996,     0.99597,     0.99594,     0.99591,     0.99586,     0.99581,     0.99576,     0.99571,     0.99566,      0.9956,\n            0.99555,      0.9955,     0.99545,      0.9954,     0.99539,     0.99538,     0.99537,     0.99537,     0.99536,     0.99535,     0.99534,     0.99533,     0.99533,     0.99532,     0.99531,      0.9953,     0.99529,     0.99528,     0.99528,     0.99527,     0.99526,     0.99525,     0.99524,\n            0.99524,     0.99523,     0.99522,     0.99521,      0.9952,     0.99519,     0.99519,     0.99518,     0.99517,     0.99516,     0.99515,     0.99515,     0.99514,     0.99513,     0.99512,     0.99511,      0.9951,      0.9951,     0.99509,     0.99508,     0.99507,     0.99506,     0.99506,\n            0.99505,     0.99504,     0.99503,     0.99502,     0.99501,     0.99501,       0.995,     0.99499,     0.99498,     0.99497,     0.99497,     0.99496,     0.99495,     0.99494,     0.99493,     0.99492,     0.99492,     0.99491,      0.9949,     0.99489,     0.99486,     0.99481,     0.99476,\n            0.99471,     0.99466,     0.99461,     0.99455,      0.9945,     0.99445,      0.9944,     0.99436,     0.99434,     0.99432,      0.9943,     0.99428,     0.99425,     0.99423,     0.99421,     0.99419,     0.99417,     0.99414,     0.99412,      0.9941,     0.99408,     0.99406,     0.99403,\n            0.99401,     0.99399,     0.99397,     0.99394,     0.99392,      0.9939,     0.99388,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,\n            0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99387,     0.99385,     0.99381,     0.99378,\n            0.99375,     0.99372,     0.99369,     0.99366,     0.99362,     0.99359,     0.99356,     0.99353,      0.9935,     0.99346,     0.99343,      0.9934,     0.99337,     0.99334,     0.99331,     0.99329,     0.99326,     0.99323,     0.99321,     0.99318,     0.99315,     0.99313,      0.9931,\n            0.99307,     0.99305,     0.99302,       0.993,     0.99297,     0.99294,     0.99292,     0.99289,     0.99286,     0.99281,     0.99265,      0.9925,     0.99234,     0.99198,      0.9918,     0.99176,     0.99172,     0.99168,     0.99164,      0.9916,     0.99156,     0.99152,     0.99148,\n            0.99144,      0.9914,     0.99136,     0.99132,      0.9912,     0.99104,     0.99089,     0.99036,     0.99019,     0.99008,     0.98997,     0.98987,     0.98978,     0.98972,     0.98967,     0.98961,     0.98955,     0.98949,     0.98944,     0.98938,     0.98932,     0.98926,     0.98926,\n            0.98926,     0.98926,     0.98926,     0.98926,     0.98926,     0.98924,     0.98915,     0.98906,     0.98897,     0.98888,     0.98879,     0.98828,     0.98773,     0.98773,     0.98773,     0.98773,     0.98726,     0.98708,     0.98693,     0.98679,     0.98671,     0.98664,     0.98656,\n            0.98648,     0.98639,     0.98631,     0.98622,     0.98613,     0.98604,     0.98594,     0.98585,     0.98575,     0.98559,     0.98525,     0.98493,     0.98456,     0.98405,     0.98385,     0.98365,     0.98337,     0.98308,      0.9828,     0.98236,     0.98207,     0.98201,     0.98196,\n             0.9819,     0.98184,     0.98179,     0.98173,     0.98168,     0.98162,      0.9815,     0.98134,     0.98117,     0.98085,     0.98039,     0.97999,     0.97904,     0.97904,     0.97904,     0.97904,     0.97904,     0.97874,     0.97796,     0.97724,     0.97666,     0.97639,     0.97621,\n            0.97604,      0.9751,      0.9723,     0.97125,     0.96937,     0.96794,     0.96644,     0.96383,     0.96189,     0.95968,      0.9577,     0.95301,     0.95088,     0.94631,     0.94357,     0.93898,     0.93524,     0.92838,     0.92529,     0.92193,     0.91638,     0.91267,     0.90476,\n              0.897,     0.89037,     0.88408,     0.87938,     0.86786,     0.85894,     0.84929,     0.83604,      0.8214,     0.79737,     0.77331,     0.74228,     0.71261,      0.6742,      0.6267,     0.57171,     0.51782,     0.46716,     0.40128,     0.33518,     0.26945,     0.20369,     0.15006,\n            0.10434,    0.072314,    0.042047,    0.027467,    0.016226,   0.0058773,   0.0025827,   0.0011562,  0.00086141,  0.00065377,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0,\n                  0,           0,           0,           0,           0,           0,           0,           0,           0,           0,           0]]), 'Confidence', 'Recall']]\nfitness: 0.9100014368683893\nkeys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\nmaps: array([    0.90056])\nnames: {0: 'object'}\nplot: True\nresults_dict: {'metrics/precision(B)': 0.9948017682519315, 'metrics/recall(B)': 0.9974437627811861, 'metrics/mAP50(B)': 0.9949434964762155, 'metrics/mAP50-95(B)': 0.9005634302452975, 'fitness': 0.9100014368683893}\nsave_dir: PosixPath('runs/detect/val2')\nspeed: {'preprocess': 7.461452484130859, 'inference': 258.749794960022, 'loss': 0.0024080276489257812, 'postprocess': 376.41379833221436}\ntask: 'detect'"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\nfrom google.colab.patches import cv2_imshow\n\nimg = cv2.imread(\"/content/drive/MyDrive/image_processing/datasets/wb_localization_dataset/images/val/nlvnpf-0137-01-045.jpg\")\n\n# Make a prediction\nresults = model.predict(source = img, imgsz = 928, save = False, stream = True)\n\n# Visualize the results\nfor result in results:\n    boxes = result.boxes  # Bounding box predictions\n    print(f\"Number of detected objects: {len(boxes)}\")\n    for box in boxes:\n        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy().astype(int)\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Draw bounding box\n\n    # Display the image with bounding boxes\n    cv2_imshow(img)","metadata":{"id":"PcCr0_CF6wlf","outputId":"e83ce543-2141-4fd6-e8f0-ce9c5ae09f14"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction","metadata":{"id":"MBDWQsYs8z7Q"}},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\nimport glob","metadata":{"id":"eQDLvuA-Copp","execution":{"iopub.status.busy":"2024-05-23T06:02:41.216593Z","iopub.execute_input":"2024-05-23T06:02:41.217378Z","iopub.status.idle":"2024-05-23T06:02:44.957895Z","shell.execute_reply.started":"2024-05-23T06:02:41.217345Z","shell.execute_reply":"2024-05-23T06:02:44.957106Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# get data\nresults = []\n\n# specify dirs\ntest_dir = '/kaggle/working/image_processing/datasets/testing'\nimage_dir = os.path.join(test_dir, 'images')\noutput_dir = os.path.join(test_dir, 'output')\n\n# cleanup output dir\ncleanup_files = glob.glob(output_dir + '/*')\nfor f in cleanup_files:\n    os.remove(f)\n\nimgs = []\nfor file in os.listdir(image_dir):\n    imgs.append(os.path.abspath(\n        os.path.join(image_dir, file)\n    ))","metadata":{"id":"72_Cd0QM85ed","execution":{"iopub.status.busy":"2024-05-22T23:37:35.334903Z","iopub.execute_input":"2024-05-22T23:37:35.335270Z","iopub.status.idle":"2024-05-22T23:37:35.343637Z","shell.execute_reply.started":"2024-05-22T23:37:35.335240Z","shell.execute_reply":"2024-05-22T23:37:35.342537Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-05-23T06:07:40.736427Z","iopub.execute_input":"2024-05-23T06:07:40.736792Z","iopub.status.idle":"2024-05-23T06:07:41.729151Z","shell.execute_reply.started":"2024-05-23T06:07:40.736765Z","shell.execute_reply":"2024-05-23T06:07:41.728256Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"image_processing  runs\twandb  yolov8n.pt  yolov8x.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"# import model\n\n# v8\nmodel_path = '/kaggle/working/image_processing/runs/detect/train2/weights/best.pt'\n# model_path = '/kaggle/working/image_processing/runs/detect/v8x_merge_aug122/weights/best.pt'\n\n# v9\n# model = YOLO('yolov9e.yaml')\n# model = YOLO('/content/drive/MyDrive/image_processing/runs/detect/train3/weights/best.pt')\n\nmodel = YOLO(model_path)\n\nmodel","metadata":{"id":"t7E-SAkADUVO","scrolled":true,"execution":{"iopub.status.busy":"2024-05-23T06:03:06.349232Z","iopub.execute_input":"2024-05-23T06:03:06.349718Z","iopub.status.idle":"2024-05-23T06:03:06.638759Z","shell.execute_reply.started":"2024-05-23T06:03:06.349688Z","shell.execute_reply":"2024-05-23T06:03:06.637910Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"YOLO(\n  (model): DetectionModel(\n    (model): Sequential(\n      (0): Conv(\n        (conv): Conv2d(3, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (1): Conv(\n        (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (2): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(400, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (3): Conv(\n        (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (4): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (5): Conv(\n        (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (6): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(2560, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-5): 6 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (7): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (8): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (9): SPPF(\n        (cv1): Conv(\n          (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n      )\n      (10): Upsample(scale_factor=2.0, mode='nearest')\n      (11): Concat()\n      (12): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (13): Upsample(scale_factor=2.0, mode='nearest')\n      (14): Concat()\n      (15): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(800, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(160, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (16): Conv(\n        (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (17): Concat()\n      (18): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(960, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (19): Conv(\n        (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n        (act): SiLU(inplace=True)\n      )\n      (20): Concat()\n      (21): C2f(\n        (cv1): Conv(\n          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (cv2): Conv(\n          (conv): Conv2d(1600, 640, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (bn): BatchNorm2d(640, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n          (act): SiLU(inplace=True)\n        )\n        (m): ModuleList(\n          (0-2): 3 x Bottleneck(\n            (cv1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (cv2): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n          )\n        )\n      )\n      (22): Detect(\n        (cv2): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(80, 64, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (cv3): ModuleList(\n          (0): Sequential(\n            (0): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n          (1-2): 2 x Sequential(\n            (0): Conv(\n              (conv): Conv2d(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (1): Conv(\n              (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n              (bn): BatchNorm2d(320, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n              (act): SiLU(inplace=True)\n            )\n            (2): Conv2d(320, 1, kernel_size=(1, 1), stride=(1, 1))\n          )\n        )\n        (dfl): DFL(\n          (conv): Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# infer\n\nresults = model.predict(source = imgs, imgsz = 928, device = [0, 1], save = False, stream = True)\n\nindex = 0\nfor result in results:\n    boxes = result.boxes.xywhn\n    labels = result.boxes.cls\n    scores = result.boxes.conf\n\n    # get file name\n    file_base_name = os.path.basename(imgs[index]).split('.jpg')[0]\n    output_file = os.path.join(output_dir, file_base_name + '.txt')\n\n    print(f'index: {index}, output to: {output_file}')\n\n    with open(output_file, 'w') as f:\n        for box, label, score in zip(boxes, labels, scores):\n            x, y, w, h = box\n            f.write(f'{label} {x} {y} {w} {h} {score}\\n')\n\n        f.close()\n\n    index += 1\n\n# for (i, img) in enumerate(imgs):\n#     results = model.predict(source = img, imgsz = 928, save = False, stream = True)","metadata":{"id":"I4GaiikqDRJ4","outputId":"9b5500a2-ca4d-49a4-d87e-c43f426cf376","execution":{"iopub.status.busy":"2024-05-22T23:37:41.842663Z","iopub.execute_input":"2024-05-22T23:37:41.843409Z","iopub.status.idle":"2024-05-22T23:37:44.520375Z","shell.execute_reply.started":"2024-05-22T23:37:41.843379Z","shell.execute_reply":"2024-05-22T23:37:44.519558Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"\n0: 928x928 66 objects, 126.8ms\n1: 928x928 187 objects, 126.8ms\n2: 928x928 176 objects, 126.8ms\n3: 928x928 140 objects, 126.8ms\n4: 928x928 190 objects, 126.8ms\n5: 928x928 242 objects, 126.8ms\n6: 928x928 235 objects, 126.8ms\n7: 928x928 204 objects, 126.8ms\n8: 928x928 225 objects, 126.8ms\n9: 928x928 300 objects, 126.8ms\nindex: 0, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0140-01-016.txt\nindex: 1, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-048.txt\nindex: 2, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-049.txt\nindex: 3, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0140-01-017.txt\nindex: 4, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-050.txt\nindex: 5, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-046.txt\nindex: 6, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0174-03-014.txt\nindex: 7, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-047.txt\nindex: 8, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0137-01-045.txt\nindex: 9, output to: /kaggle/working/image_processing/datasets/testing/output/nlvnpf-0174-03-013.txt\nSpeed: 4.7ms preprocess, 126.8ms inference, 1.2ms postprocess per image at shape (1, 3, 928, 928)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\ntest_dir = '/kaggle/working/image_processing/datasets/testing'\nimage_dir = os.path.join(test_dir, 'images')\noutput_dir = os.path.join(test_dir, 'output')\nground_truth_dir = os.path.join(test_dir, 'ground_truth')\n\n!python /kaggle/working/image_processing/task1.py --gt {ground_truth_dir} --pred {output_dir} --img {image_dir}","metadata":{"execution":{"iopub.status.busy":"2024-05-22T23:37:46.755292Z","iopub.execute_input":"2024-05-22T23:37:46.755680Z","iopub.status.idle":"2024-05-22T23:38:18.929332Z","shell.execute_reply.started":"2024-05-22T23:37:46.755648Z","shell.execute_reply":"2024-05-22T23:38:18.928241Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"0.996851415357012 0.5\n99.6851415357012\n0.9958197039511334 0.55\n99.58197039511334\n0.9952797205323592 0.6\n99.52797205323593\n0.9931505866210759 0.65\n99.31505866210759\n0.9900759252659408 0.7\n99.00759252659408\n0.9874352637574949 0.75\n98.7435263757495\n0.9843549234964581 0.8\n98.43549234964581\n0.9573320940579377 0.85\n95.73320940579377\n0.7790060735677617 0.9\n77.90060735677618\n0.1001713774187753 0.95\n10.01713774187753\n0.8779477084025948\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Export","metadata":{}},{"cell_type":"code","source":"!rm -rf /kaggle/working/output.zip","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r /kaggle/working/output.zip /kaggle/working/image_processing","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]}]}